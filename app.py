import os, json, time
from pathlib import Path

import streamlit as st
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import google.generativeai as genai

# â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€
EMBED_MODEL = "models/embedding-001"
HIGH_TH     = 0.80   # å®Œå…¨ä¸€è‡´
MID_TH      = 0.30   # å‚è€ƒã«ãªã‚‹
LOG_DIR     = Path("logs"); LOG_DIR.mkdir(exist_ok=True)
CHAT_LOG    = LOG_DIR / "chat_logs.jsonl"

# â”€â”€â”€â”€â”€ UTILS â”€â”€â”€â”€â”€
def log_line(path: Path, row: dict):
    safe = {k: (v if isinstance(v,(str,int,float,bool,type(None))) else str(v))
            for k,v in row.items()}
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(safe, ensure_ascii=False) + "\n")

# â”€â”€â”€â”€â”€ API KEY â”€â”€â”€â”€â”€
load_dotenv(); key = os.getenv("GOOGLE_API_KEY")
if not key:  st.error("GOOGLE_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„"); st.stop()
genai.configure(api_key=key)

# â”€â”€â”€â”€â”€ FAQ èª­è¾¼ â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="FAQ ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦")
def load_faq(csv="faq.csv"):
    df = pd.read_csv(csv)
    vecs = [
        genai.embed_content(model=EMBED_MODEL,
                            content=q,
                            task_type="retrieval_query")["embedding"]
        for q in df["question"]
    ]
    vecs = np.asarray(vecs, dtype="float32")
    vecs /= np.linalg.norm(vecs, axis=1, keepdims=True)
    return df, vecs
faq_df, faq_vecs = load_faq()

def best_match(q_vec):
    q = q_vec / np.linalg.norm(q_vec)
    sims = faq_vecs @ q
    idx  = int(sims.argmax())
    return float(sims[idx]), idx

# â”€â”€â”€â”€â”€ UI â”€â”€â”€â”€â”€
st.set_page_config("AIå…ˆè¼© FAQ Bot", "ðŸ¤–")
st.title("ðŸŽ“ AIå…ˆè¼© â€“ FAQãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
if "hist" not in st.session_state: st.session_state.hist=[]
for r,t in st.session_state.hist: st.chat_message(r).markdown(t)

# â”€â”€â”€â”€â”€ LOOP â”€â”€â”€â”€â”€
if q := st.chat_input("è³ªå•ã‚’ã©ã†ãž"):
    st.chat_message("user").markdown(q)
    st.session_state.hist.append(("user", q))

    q_vec = genai.embed_content(model=EMBED_MODEL,
                                content=q,
                                task_type="retrieval_query")["embedding"]
    q_vec = np.asarray(q_vec, dtype="float32")
    sim, idx = best_match(q_vec)
    faq_q, faq_a = faq_df.iloc[idx][["question","answer"]]

    if sim >= HIGH_TH:                  # â‘  å®Œå…¨ä¸€è‡´
        ans = f"{faq_a}\n\nã¾ãŸä½•ã‹ã‚ã£ãŸã‚‰é æ…®ãªãèžã„ã¦ã­ï¼"
    elif sim >= MID_TH:                 # â‘¡ å‚è€ƒã«ãªã‚‹
        ans = (f"å‚è€ƒã«ãªã‚Šãã†ãªæƒ…å ±ã ã‚ˆï¼š\n\n{faq_a}\n\n"
               "å¿µã®ãŸã‚ä»–ã®å…ˆè¼©ã«ã‚‚ç¢ºèªã—ã¦ã¿ã¦ï¼")
    else:                               # â‘¢ åˆ†ã‹ã‚‰ãªã„
        ans = "ã”ã‚ã‚“ã€ã„ã¾ã¯ãã®æƒ…å ±ã‚’æŒã£ã¦ã„ãªã„ã‚“ã ã€‚ä»–ã®å…ˆè¼©ã«ã‚‚èžã„ã¦ã¿ã¦ã­ã€‚"

    st.chat_message("assistant").markdown(ans)
    st.session_state.hist.append(("assistant", ans))
    st.caption(f"ã‚³ã‚µã‚¤ãƒ³é¡žä¼¼åº¦: {sim:.2f}")

    log_line(CHAT_LOG, {"ts":time.time(),"q":q,"a":ans,"sim":sim,"faq_q":faq_q if sim>=MID_TH else ""})
