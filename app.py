# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ app.py  (cost ç„¡è¦–ãƒ»Flash å…¨æŠ•ã’ç‰ˆ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, json, time
from pathlib import Path

import streamlit as st
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import google.generativeai as genai

# â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€
EMBED_MODEL = "models/embedding-001"
CHAT_MODEL  = "models/gemini-1.5-flash-latest"
TOP_K       = 10         # Flash ã«æ¸¡ã™ FAQ ä»¶æ•°
LOG_DIR     = Path("logs"); LOG_DIR.mkdir(exist_ok=True)
CHAT_LOG    = LOG_DIR / "chat_logs.jsonl"

# â”€â”€â”€â”€â”€â”€ UTIL â”€â”€â”€â”€â”€â”€
def jl_append(path: Path, row: dict):
    safe = {k:(v if isinstance(v,(str,int,float,bool,type(None))) else str(v)) for k,v in row.items()}
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(safe, ensure_ascii=False) + "\n")

# â”€â”€â”€â”€â”€â”€ API KEY â”€â”€â”€â”€â”€â”€
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY ã‚’ .env ã‹ Secrets ã§è¨­å®šã—ã¦ãã ã•ã„"); st.stop()
genai.configure(api_key=api_key)
MODEL = genai.GenerativeModel(CHAT_MODEL)

# â”€â”€â”€â”€â”€â”€ FAQ èª­ã¿è¾¼ã¿ & åŸ‹ã‚è¾¼ã¿ â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="FAQ ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦")
def load_faq(csv="faq.csv"):
    df = pd.read_csv(csv)
    if not {"question","answer"}.issubset(df.columns):
        st.error("faq.csv ã« question / answer åˆ—ãŒå¿…è¦"); st.stop()

    doc_vecs = [
        genai.embed_content(
            model=EMBED_MODEL,
            content=row["question"],
            task_type="retrieval_document"
        )["embedding"]
        for _, row in df.iterrows()
    ]
    doc_vecs = np.asarray(doc_vecs, dtype="float32")
    doc_vecs /= np.linalg.norm(doc_vecs, axis=1, keepdims=True)
    return df, doc_vecs
faq_df, faq_vecs = load_faq()

def top_k(q_vec, k=TOP_K):
    q = q_vec / np.linalg.norm(q_vec)
    sims = faq_vecs @ q
    idxs = sims.argsort()[-k:][::-1]
    return sims[idxs], idxs

# â”€â”€â”€â”€â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€
st.set_page_config("AIå…ˆè¼© FAQ Bot", "ğŸ¤–")
st.title("ğŸ“ AIå…ˆè¼© â€“ FAQãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
if "hist" not in st.session_state: st.session_state.hist=[]
for r,t in st.session_state.hist: st.chat_message(r).markdown(t)

# â”€â”€â”€â”€â”€â”€ ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ— â”€â”€â”€â”€â”€â”€
if q := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
    st.chat_message("user").markdown(q)
    st.session_state.hist.append(("user", q))

    # 1) è³ªå•ã‚’ query åŸ‹ã‚è¾¼ã¿
    q_vec = genai.embed_content(
        model=EMBED_MODEL,
        content=q,
        task_type="retrieval_query"
    )["embedding"]
    q_vec = np.asarray(q_vec, dtype="float32")

    # 2) FAQ ä¸Šä½ k ä»¶
    sims, idxs = top_k(q_vec)
    faq_block = "\n\n".join(
        f"{i+1}. (cos={sims[i]:.2f})\nQ:{faq_df.iloc[idx]['question']}\nA:{faq_df.iloc[idx]['answer']}"
        for i,idx in enumerate(idxs)
    )

    # 3) Flash ã¸ä¸¸æŠ•ã’
    prompt = (
        "ã‚ãªãŸã¯å¤§å­¦ã®é ¼ã‚Œã‚‹å…ˆè¼©ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚\n"
        "ä»¥ä¸‹ã®å€™è£œ FAQ ã‚’èª­ã¿ã€è³ªå•ã«æœ€ã‚‚åˆè‡´ã™ã‚‹å›ç­”ãŒã‚ã‚Œã°ã€Œãã®å›ç­”ã ã‘ã€ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚\n"
        "è©²å½“ãŒç„¡ã„å ´åˆã¯ **NO_ANSWER** ã¨ã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"ã€è³ªå•ã€‘\n{q}\n\nã€å€™è£œFAQã€‘\n{faq_block}"
    )
    try:
        resp    = MODEL.generate_content(prompt)
        raw_ans = resp.text.strip()
    except Exception as e:
        raw_ans = "NO_ANSWER"
        st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {type(e).__name__} â€“ {e}")

    # 4) å‡ºåŠ›åˆ¤å®š
    if raw_ans == "NO_ANSWER":
        final = "ã”ã‚ã‚“ã€ä»Šã¯ãã®æƒ…å ±ã‚’æŒã£ã¦ã„ãªã„ã‚“ã ã€‚ä»–ã®å…ˆè¼©ã«ã‚‚èã„ã¦ã¿ã¦ã­ã€‚"
        hit   = False
    else:
        final = f"{raw_ans}\n\nã¾ãŸä½•ã‹ã‚ã£ãŸã‚‰é æ…®ãªãèã„ã¦ã­ï¼"
        hit   = True

    # 5) è¡¨ç¤º & ãƒ­ã‚°
    st.chat_message("assistant").markdown(final)
    st.session_state.hist.append(("assistant", final))
    jl_append(CHAT_LOG, {"ts":time.time(),"q":q,"a":final,"faq_hit":hit})
