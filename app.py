import os, json, time
from pathlib import Path

import streamlit as st
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import google.generativeai as genai

# ---------- CONFIG ----------
EMBED_MODEL = "models/embedding-001"
CHAT_MODEL  = "models/gemini-1.5-flash-latest"

TOP_K       = 5      # FAQ ã‚’æ¸¡ã™ä¸Šä½ä»¶æ•°
SIM_FLOOR   = 0.15   # ã“ã‚Œæœªæº€ã¯ FAQ ã‚’ç„¡ç†ã«æ¸¡ã•ãªã„
LOG_DIR     = Path("logs"); LOG_DIR.mkdir(exist_ok=True)
CHAT_LOG    = LOG_DIR / "chat_logs.jsonl"

# ---------- UTILS -----------
def save_jsonl(path: Path, data: dict):
    safe = {k: (v if isinstance(v,(str,int,float,bool,type(None))) else str(v))
            for k,v in data.items()}
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(safe, ensure_ascii=False) + "\n")

# ---------- API KEY ----------
load_dotenv(); key = os.getenv("GOOGLE_API_KEY")
if not key: st.error("GOOGLE_API_KEY ãŒæœªè¨­å®šã§ã™"); st.stop()
genai.configure(api_key=key)

# ---------- FAQ LOAD ---------
@st.cache_resource(show_spinner="FAQ ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦")
def load_faq(csv="faq.csv"):
    df = pd.read_csv(csv)
    vecs = [
        genai.embed_content(model=EMBED_MODEL,
                            content=q,
                            task_type="retrieval_query")["embedding"]
        for q in df["question"]
    ]
    vecs = np.asarray(vecs, dtype="float32")
    vecs /= np.linalg.norm(vecs, axis=1, keepdims=True)
    return df, vecs
faq_df, faq_vecs = load_faq()

def top_k(q_vec, k=TOP_K):
    q = q_vec / np.linalg.norm(q_vec)
    sims = faq_vecs @ q
    idx  = sims.argsort()[-k:][::-1]
    return sims[idx], idx

# ---------- STREAMLIT UI -----
st.set_page_config("AIå…ˆè¼© FAQ Bot", "ğŸ¤–")
st.title("ğŸ“ AIå…ˆè¼© â€“ FAQãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

if "hist" not in st.session_state: st.session_state.hist=[]
for r,t in st.session_state.hist: st.chat_message(r).markdown(t)

if q := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
    st.chat_message("user").markdown(q)
    st.session_state.hist.append(("user", q))

    q_vec = genai.embed_content(model=EMBED_MODEL,
                                content=q,
                                task_type="retrieval_query")["embedding"]
    q_vec = np.asarray(q_vec, dtype="float32")
    sims, idxs = top_k(q_vec)

    # ---- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ ----------------------
    faqs = [
        f"{i+1}. Q: {faq_df.iloc[idx]['question']}\n   A: {faq_df.iloc[idx]['answer']}"
        for i,idx in enumerate(idxs) if sims[i] >= SIM_FLOOR
    ]
    prompt = (
        "ã‚ãªãŸã¯å¤§å­¦ã®é ¼ã‚Œã‚‹å…ˆè¼©ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚\n"
        "ä»¥ä¸‹ã®å€™è£œ FAQ ã‚’èª­ã‚“ã§ï¼Œè³ªå•ã«æœ€ã‚‚åˆã†å›ç­”ã ã‘ã‚’é¸ã³ï¼Œ"
        "ãã®å›ç­”æ–‡ã ã‘ã‚’è¿”ç­”ã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "ã‚‚ã—è©²å½“ã™ã‚‹ã‚‚ã®ãŒä¸€ã¤ã‚‚ç„¡ã„å ´åˆã¯ **NO_ANSWER** ã¨ã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"ã€è³ªå•ã€‘\n{q}\n\nã€å€™è£œFAQã€‘\n" + "\n".join(faqs)
    )

    # ---- Gemini å‘¼ã³å‡ºã— --------------------
    try:
        rsp = genai.generate_content(
            model=CHAT_MODEL,
            contents=[{"role":"user","parts":[{"text":prompt}]}],
        )
        ans_raw = rsp.candidates[0].content.parts[0].text.strip()
    except Exception as e:
        ans_raw = "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ã§å›ç­”ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦è©¦ã—ã¦ãã ã•ã„ã€‚"

    if ans_raw == "NO_ANSWER":
        answer = "ã”ã‚ã‚“ã€ä»Šã¯ãã®æƒ…å ±ã‚’æŒã£ã¦ã„ãªã„ã‚“ã ã€‚ã»ã‹ã®å…ˆè¼©ã«ã‚‚èã„ã¦ã¿ã¦ã­ã€‚"
        faq_hit = False
    else:
        answer = f"{ans_raw}\n\nå›°ã£ãŸã‚‰ã„ã¤ã§ã‚‚ã¾ãŸèã„ã¦ã­ã€‚"
        faq_hit = True

    st.chat_message("assistant").markdown(answer)
    st.session_state.hist.append(("assistant", answer))

    save_jsonl(CHAT_LOG, {"ts":time.time(),"q":q,"a":answer,"faq_hit":faq_hit})
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
