# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  app.py  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, json, time, logging
from pathlib import Path

import streamlit as st
import pandas as pd
import numpy as np
import faiss
from dotenv import load_dotenv
import google.generativeai as genai

# ----------------- CONFIG ------------------ #
EMBED_MODEL = "models/embedding-001"
CHAT_MODEL  = "models/gemini-1.5-flash-latest"
COS_THRESHOLD = 0.30          # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®é–¾å€¤
TOP_K = 3                     # FAQ ä¸Šä½ä½•ä»¶ã‚’è¦‹ã‚‹ã‹

LOG_DIR = Path("logs"); LOG_DIR.mkdir(exist_ok=True)
CHAT_LOG_FILE      = LOG_DIR / "chat_logs.jsonl"
UNANSWERED_LOGFILE = LOG_DIR / "unanswered.jsonl"

st.set_page_config("AIå…ˆè¼© FAQ Bot", "ğŸ¤–")
st.title("ğŸ“ AIå…ˆè¼© â€“ FAQãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

# -------------- UTILï¼ˆå®‰å…¨ãƒ­ã‚°ï¼‰ ------------- #
def append_jsonl(path: Path, data: dict) -> None:
    def to_safe(x):
        return x if isinstance(x, (str, int, float, bool, type(None))) else str(x)
    safe = {k: to_safe(v) for k, v in data.items()}
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(safe, ensure_ascii=False) + "\n")

# -------------- ç’°å¢ƒå¤‰æ•° --------------------- #
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    st.error("`.env` ã‹ Secrets ã« GOOGLE_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„"); st.stop()
genai.configure(api_key=API_KEY)

# -------------- FAQ èª­ã¿è¾¼ã¿ï¼†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ --- #
@st.cache_resource(show_spinner="FAQ ã‚’èª­ã¿è¾¼ã¿ä¸­ â€¦")
def load_faq(csv_path="faq.csv"):
    if not Path(csv_path).exists():
        st.error(f"{csv_path} ãŒã‚ã‚Šã¾ã›ã‚“"); st.stop()

    df = pd.read_csv(csv_path)
    if not {"question", "answer"}.issubset(df.columns):
        st.error("faq.csv ã« 'question','answer' åˆ—ãŒå¿…è¦"); st.stop()

    # Embedding & Cosine ç”¨å‰å‡¦ç†
    vecs = [genai.embed_content(model=EMBED_MODEL,
                                content=q,
                                task_type="retrieval_query")["embedding"]
            for q in df["question"]]

    vecs = np.array(vecs).astype("float32")
    faiss.normalize_L2(vecs)              # â˜… æ­£è¦åŒ–
    index = faiss.IndexFlatIP(vecs.shape[1])
    index.add(vecs)
    return df, index

faq_df, faq_index = load_faq()

# -------------- SIDEBAR --------------------- #
with st.sidebar:
    st.write(f"ğŸ“„ FAQ ä»¶æ•° : {len(faq_df)}")
    if st.button("æœ€åˆã®5ä»¶ã‚’è¦‹ã‚‹"):
        st.dataframe(faq_df.head())
    COS_THRESHOLD = st.slider("ã‚³ã‚µã‚¤ãƒ³é–¾å€¤", 0.0, 1.0, COS_THRESHOLD, 0.01)

# -------------- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ ----------------- #
if "history" not in st.session_state: st.session_state.history = []

for role, text in st.session_state.history:
    st.chat_message(role).markdown(text)

# -------------- ãƒ¦ãƒ¼ã‚¶å…¥åŠ› ------------------- #
if user_q := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
    st.session_state.history.append(("user", user_q))
    st.chat_message("user").markdown(user_q)

    # --- Embedding & æ¤œç´¢ï¼ˆã‚³ã‚µã‚¤ãƒ³ï¼‰ -------- #
    uvec = genai.embed_content(model=EMBED_MODEL,
                               content=user_q,
                               task_type="retrieval_query")["embedding"]
    uvec = np.asarray(uvec, dtype="float32"); faiss.normalize_L2(uvec)
    D, I = faq_index.search(uvec[None], TOP_K)   # D ã¯ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
    best_sim, best_idx = float(D[0][0]), int(I[0][0])

    use_faq = best_sim >= COS_THRESHOLD
    answer, src_q = "", ""

    if use_faq:
        row = faq_df.iloc[best_idx]
        src_q, answer = row["question"], row["answer"]
    else:
        # ---- Gemini ç”Ÿæˆ -------------------- #
        context = ""
        if best_sim > 0.1:
            row = faq_df.iloc[best_idx]
            context = f"å‚è€ƒFAQ:\nQ: {row['question']}\nA: {row['answer']}\n---\n"
        system_prompt = (
            'ã‚ãªãŸã¯å¤§å­¦ã®å…ˆè¼©ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ "AIå…ˆè¼©" ã§ã™ã€‚'
            'è³ªå•ã«æ—¥æœ¬èªã§ç«¯çš„ã‹ã¤ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚'
        )
        full_prompt = f"{context}ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_q}"
        try:
            resp = genai.generate_content(
                model=CHAT_MODEL,
                contents=[
                    {"role": "system", "parts": [{"text": system_prompt}]},
                    {"role": "user",   "parts": [{"text": full_prompt}]},
                ],
            )
            answer = resp.candidates[0].content.parts[0].text
        except Exception as e:
            answer = "å›ç­”ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ç½®ã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
            logging.exception(e)

    # ---- ç”»é¢è¡¨ç¤º --------------------------- #
    st.chat_message("assistant").markdown(answer)
    st.session_state.history.append(("assistant", answer))
    st.caption(f"ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {best_sim:.2f} / FAQãƒãƒƒãƒ: {use_faq}")

    # ---- ãƒ­ã‚°æ›¸ãè¾¼ã¿ ----------------------- #
    log = {
        "ts"       : time.time(),
        "question" : user_q,
        "answered_by_faq": use_faq,
        "similarity": best_sim,
        "faq_question": src_q,
        "answer"    : answer,
    }
    append_jsonl(CHAT_LOG_FILE, log)
    if not use_faq: append_jsonl(UNANSWERED_LOGFILE, log)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.caption(f"ï¼ˆé¡ä¼¼åº¦: {top_similarity:.2f}, æœªå›ç­”: {is_unanswered}ï¼‰")
