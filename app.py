import os, json, time
from pathlib import Path

import streamlit as st
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import google.generativeai as genai

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMBED_MODEL = "models/embedding-001"
CHAT_MODEL  = "models/gemini-1.5-flash-latest"

TOP_K      = 5      # LLM ã«æ¸¡ã™ FAQ ä»¶æ•°
SIM_CUTOFF = 0.15   # ã“ã‚Œæœªæº€ã¯ FAQ ã‚’æ¸¡ã•ãªã„
HIGH_TH    = 0.80   # ã€Œå®Œå…¨ä¸€è‡´ã€ã¨ã¿ãªã™å¢ƒç•Œ
MID_TH     = 0.30   # ã€Œå‚è€ƒã«ãªã‚‹ã€å¢ƒç•Œ

LOG_DIR  = Path("logs"); LOG_DIR.mkdir(exist_ok=True)
CHAT_LOG = LOG_DIR / "chat_logs.jsonl"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UTILS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def jsonl_append(path: Path, row: dict):
    safe = {
        k: v if isinstance(v, (str, int, float, bool, type(None))) else str(v)
        for k, v in row.items()
    }
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(safe, ensure_ascii=False) + "\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENV & API KEY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY ã‚’ .env ã¾ãŸã¯ Secrets ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()
genai.configure(api_key=api_key)

# ãƒ¢ãƒ‡ãƒ«ã¯ configure å¾Œã«ç”Ÿæˆ
MODEL = genai.GenerativeModel(CHAT_MODEL)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FAQ èª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner="FAQ ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦")
def load_faq(csv_path: str = "faq.csv"):
    if not Path(csv_path).exists():
        st.error(f"{csv_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"); st.stop()

    df = pd.read_csv(csv_path)
    if not {"question", "answer"}.issubset(df.columns):
        st.error("faq.csv ã« 'question','answer' åˆ—ãŒå¿…è¦ã§ã™"); st.stop()

    vecs = [
        genai.embed_content(
            model=EMBED_MODEL,
            content=q,
            task_type="retrieval_query"
        )["embedding"]
        for q in df["question"]
    ]
    vecs = np.asarray(vecs, dtype="float32")
    vecs /= np.linalg.norm(vecs, axis=1, keepdims=True)  # L2 æ­£è¦åŒ–
    return df, vecs

faq_df, faq_vecs = load_faq()

def top_k_cosine(q_vec, k: int = TOP_K):
    q = q_vec / np.linalg.norm(q_vec)
    sims = faq_vecs @ q
    idxs = sims.argsort()[-k:][::-1]
    return sims[idxs], idxs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ STREAMLIT UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("AIå…ˆè¼© FAQ Bot", "ğŸ¤–")
st.title("ğŸ“ AIå…ˆè¼© â€“ FAQãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")

if "history" not in st.session_state:
    st.session_state.history = []

for role, msg in st.session_state.history:
    st.chat_message(role).markdown(msg)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if user_q := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
    st.chat_message("user").markdown(user_q)
    st.session_state.history.append(("user", user_q))

    # 1) åŸ‹ã‚è¾¼ã¿
    q_vec = genai.embed_content(
        model=EMBED_MODEL,
        content=user_q,
        task_type="retrieval_query"
    )["embedding"]
    q_vec = np.asarray(q_vec, dtype="float32")

    # 2) é¡ä¼¼ FAQ ã‚’å–å¾—
    sims, idxs = top_k_cosine(q_vec)

    # 3) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    faq_lines = [
        f"{i+1}. Q: {faq_df.iloc[idx]['question']}\n   A: {faq_df.iloc[idx]['answer']}"
        for i, idx in enumerate(idxs)
        if sims[i] >= SIM_CUTOFF
    ]
    prompt = (
        "ã‚ãªãŸã¯å¤§å­¦ã®é ¼ã‚Œã‚‹å…ˆè¼©ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚\n"
        "æ¬¡ã®å€™è£œ FAQ ã‚’å‚è€ƒã«ã€è³ªå•ã«æœ€ã‚‚åˆã†å›ç­”ã ã‘ã‚’è¿”ç­”ã—ã¦ãã ã•ã„ã€‚\n"
        "è©²å½“ã™ã‚‹ã‚‚ã®ãŒç„¡ã‘ã‚Œã° **NO_ANSWER** ã¨ã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"ã€è³ªå•ã€‘\n{user_q}\n\nã€å€™è£œFAQã€‘\n"
        + ("\n".join(faq_lines) if faq_lines else "ï¼ˆå€™è£œãªã—ï¼‰")
    )

    # 4) Gemini Flash ã§å›ç­”é¸æŠ
    try:
        resp   = MODEL.generate_content(prompt)
        raw_ans = resp.text.strip()
    except Exception as e:
        raw_ans = "NO_ANSWER"
        st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {type(e).__name__} â€“ {e}")

    # 5) 3 æ®µéšã§æœ€çµ‚æ•´å½¢
    if raw_ans == "NO_ANSWER":
        answer  = "ã”ã‚ã‚“ã€ã„ã¾ã¯ãã®æƒ…å ±ã‚’æŒã£ã¦ã„ãªã„ã‚“ã ã€‚ä»–ã®å…ˆè¼©ã«ã‚‚èã„ã¦ã¿ã¦ã­ã€‚"
        faq_hit = False
    elif sims[0] >= HIGH_TH:
        answer  = f"{raw_ans}\n\nã¾ãŸä½•ã‹ã‚ã£ãŸã‚‰é æ…®ãªãèã„ã¦ã­ï¼"
        faq_hit = True
    else:
        answer  = f"{raw_ans}\n\nå¿µã®ãŸã‚ä»–ã®å…ˆè¼©ã«ã‚‚ç¢ºèªã—ã¦ã¿ã¦ï¼"
        faq_hit = True

    # 6) è¡¨ç¤ºï¼†å±¥æ­´
    st.chat_message("assistant").markdown(answer)
    st.session_state.history.append(("assistant", answer))

    # 7) ãƒ­ã‚°ä¿å­˜
    jsonl_append = jsonl_append if 'jsonl_append' in globals() else log_jsonl
    jsonl_append(
        CHAT_LOG,
        {
            "ts": time.time(),
            "question": user_q,
            "answer": answer,
            "faq_hit": faq_hit,
            "top_similarity": float(sims[0] if sims.size else 0),
        },
    )
